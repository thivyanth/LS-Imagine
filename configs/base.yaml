use_wandb: true
# Optional natural-language prompt (e.g. LS-Imagine affordance prompt). Stored in W&B config if provided.
prompt: ""
logdir: null
traindir: null
evaldir: null
offline_traindir: ""
offline_evaldir: ""
seed: 0
deterministic_run: false
steps: 1e6
parallel: false
eval_every: 20000
eval_episode_num: 10
log_every: 20000
reset_every: 0
device: cuda:0
compile: true
precision: 32
debug: false
video_pred_log: true

# Environment
task: minedojo
size: [64, 64]
envs: 1
action_repeat: 1
time_limit: 1000
grayscale: false
prefill: 2500
reward_EMA: true

# Model
dyn_hidden: 512
dyn_deter: 512
dyn_stoch: 32
dyn_discrete: 32
dyn_rec_depth: 1
dyn_mean_act: none
dyn_std_act: sigmoid2
dyn_min_std: 0.1
grad_heads: [decoder, reward, end, intrinsic, jump, jumping_steps, accumulated_reward]
units: 512
act: SiLU
norm: true
encoder:
  mlp_keys: "$^"
  cnn_keys: image
  act: SiLU
  norm: true
  cnn_depth: 32
  kernel_size: 4
  minres: 4
  mlp_layers: 5
  mlp_units: 1024
  symlog_inputs: true
decoder:
  mlp_keys: "$^"
  cnn_keys: image
  act: SiLU
  norm: true
  cnn_depth: 32
  kernel_size: 4
  minres: 4
  mlp_layers: 5
  mlp_units: 1024
  cnn_sigmoid: false
  image_dist: mse
  vector_dist: symlog_mse
  outscale: 1.0
actor:
  layers: 2
  dist: normal
  entropy: 3e-4
  unimix_ratio: 0.01
  std: learned
  min_std: 0.1
  max_std: 1.0
  temp: 0.1
  lr: 3e-5
  eps: 1e-5
  grad_clip: 100.0
  outscale: 1.0
critic:
  layers: 2
  dist: symlog_disc
  slow_target: true
  slow_target_update: 1
  slow_target_fraction: 0.02
  lr: 3e-5
  eps: 1e-5
  grad_clip: 100.0
  outscale: 0.0
reward_head:
  layers: 2
  dist: symlog_disc
  loss_scale: 1.0
  outscale: 0.0
end_head:
  layers: 2
  loss_scale: 1.0
  outscale: 1.0
jump_head:
  layers: 2
  loss_scale: 1.0
  outscale: 1.0
concentration_score_head:
  layers: 2
  dist: symlog_disc
  loss_scale: 1.0
  outscale: 0.0
intrinsic_head:
  layers: 2
  dist: symlog_disc
  loss_scale: 1.0
  outscale: 0.0
jumping_steps_head:
  layers: 2
  dist: symlog_disc
  loss_scale: 1.0
  outscale: 0.0
accumulated_reward_head:
  layers: 2
  dist: symlog_disc
  loss_scale: 1.0
  outscale: 0.0
dyn_scale: 0.5
rep_scale: 0.1
kl_free: 1.0
img_weight: 1.0
jmp_weight: 1.0
weight_decay: 0.0
unimix_ratio: 0.01
initial: learned
baseline_mode: false

# --- VLM-only baseline LoRA (training-time MineCLIP embedding recompute) ---
# NOTE: env-side MineCLIP reward shaping remains no-grad and wrapper-based; these flags only affect
# whether training recomputes embeddings from stored raw frames.
store_vlm_rgb: false
vlm_rgb_key: vlm_rgb
vlm_recompute_mc_e: false
vlm_train_stride: 1           # compute VLM embedding with grad every k timesteps
vlm_grad_checkpoint: false     # activation checkpointing inside MineCLIP vision transformer blocks
vlm_precision: fp32            # {fp32, fp16, bf16}

vlm_lora_enable: false
lora_r: 8
lora_alpha: 16
lora_dropout: 0.0
lora_target_modules: []

# MPF-LSD (MineCLIP-Pixel Fusion Long-Short Dreamer): optional posterior fusion.
mc_fuse: false
mc_fuse_zoomed: true
mc_layers: 2
mc_dmc: 0
mc_gate_init: 0.1
mc_use_ln: true

# Training
batch_size: 16
batch_length: 32
train_ratio: 512
pretrain: 100
model_lr: 1e-4
opt_eps: 1e-8
grad_clip: 1000
dataset_size: 1000000
opt: adam

# Behavior
discount: 0.997
discount_lambda: 0.95
jump_prob: 0.7
jump_prob_decay: 0.9997
imag_horizon: 15
max_imag_sequences_num: 1024
imag_gradient: dynamics
imag_gradient_mix: 0.0
eval_state_mean: false

# Exploration
expl_behavior: greedy
expl_until: 0
expl_extr_scale: 0.0
expl_intr_scale: 1.0
disag_target: stoch
disag_log: true
disag_models: 10
disag_offset: 1
disag_layers: 4
disag_units: 400
disag_action_cond: false

